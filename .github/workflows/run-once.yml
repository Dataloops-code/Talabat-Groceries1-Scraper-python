name: Run Once - {area_name}

on:
  schedule:
    - cron: "0 */6 * * *"  # Runs every 6 hours
  workflow_dispatch:     # Allows manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hours, adjust as needed
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: master  # Adjust if your default branch is 'main'

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright Browsers
        run: |
          python -m playwright install chromium

      - name: Install Playwright System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libdbus-glib-1-2 libgtk-3-0 libgdk-pixbuf2.0-0 libxcomposite1 libxdamage1 libxrandr2 libgbm1 libpcre3 libwoff1 libevent-2.1-7 libopus0 libsecret-1-0 libhyphen0 libgles2 libsoup2.4-1 libvpx-dev

      - name: Restore cached progress
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: |
            current_progress_{area_name}.json
            scraped_progress_{area_name}.json
            output/{area_name}.json
            output/{area_name}_detailed.xlsx
          key: talabat-groceries-progress-{area_name}-${{ github.run_id }}
          restore-keys: |
            talabat-groceries-progress-{area_name}-

      - name: Check if area is completed
        id: check-completion
        run: |
          if [ -f "current_progress_{area_name}.json" ]; then
            if jq -e '.completed_areas[] | select(. == "{area_name}")' current_progress_{area_name}.json > /dev/null; then
              echo "completed=true" >> $GITHUB_OUTPUT
            else
              echo "completed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "completed=false" >> $GITHUB_OUTPUT
          fi

      - name: Run Talabat Groceries Scraper
        if: steps.check-completion.outputs.completed == 'false'
        run: |
          generate_workflows.py
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          TALABAT_GCLOUD_KEY_JSON: ${{ secrets.TALABAT_GCLOUD_KEY_JSON }}

      - name: Debug file presence
        if: always()
        run: |
          ls -la
          cat current_progress_{area_name}.json || echo "current_progress_{area_name}.json not found"
          cat scraped_progress_{area_name}.json || echo "scraped_progress_{area_name}.json not found"
          ls output/{area_name}_detailed.xlsx || echo "{area_name}_detailed.xlsx not found"

      - name: Commit progress updates
        if: always()
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "action@github.com"
          git add current_progress_{area_name}.json scraped_progress_{area_name}.json output/*.json output/*.xlsx || true
          git commit -m "Update scraper progress for {area_name} run ${{ github.run_id }}" || echo "No changes to commit"
          for attempt in 1 2 3; do
            git pull --rebase && git push && break
            echo "Git push failed (attempt $attempt/3), retrying..."
            sleep $((attempt * 5))
          done || echo "Failed to push after 3 attempts"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Save progress to cache
        if: always()
        uses: actions/cache/save@v4
        with:
          path: |
            current_progress_{area_name}.json
            scraped_progress_{area_name}.json
            output/{area_name}.json
            output/{area_name}_detailed.xlsx
          key: talabat-groceries-progress-{area_name}-${{ github.run_id }}

      - name: Upload progress artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: talabat-groceries-progress-{area_name}
          path: |
            current_progress_{area_name}.json
            scraped_progress_{area_name}.json
            output/{area_name}.json
            output/{area_name}_detailed.xlsx
            scraper.log
          retention-days: 7
          if-no-files-found: warn

      - name: Cleanup
        if: always()
        run: |
          rm -rf ~/.cache/ms-playwright
